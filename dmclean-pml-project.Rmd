---
title: "Classifying Movement in a Weight Lifting HAR Study"
author: "Douglas McLean, Peer Assessment, Machine Learning, Coursera"
date: "30th January 2016"
output: html_document
---


# Exective Summary
The objective of this peer assessed machine learning task was to use the [Weight Lifting Exercise Dataset](http://groupware.les.inf.puc-rio.br/har) to classify outcome category: `A`,...,`E`; on a number of weightlifting exercises from a number of participants. Data was taken from the 6 participants who had accelerometers fitted to their belt, forearm, arm, and dumbell and were asked to perform barbell lifts correctly and incorrectly in 5 different ways. The [training data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv) and the [test data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv) was downloaded from the coursera website.

This report is organised as follows. After loading the data, it was pre-processed to reduce the total number of predictor variables to a reasonable number whilst, in the process, dealing with missing data and performing some variable transformations. The training data was then split into training and test sets (with the test set further divided into two). Three models were then fitted: a gradient boosted classification tree model, a single hidden layer neural network (perceptron) model and a blended model of the first two. 10-fold cross-validation was used for hyper-parameter tuning throughout. Predictions were then made and the models assessed out-of-sample. 

Summarising the performance of the models: In-sample, the gradient boosted classification tree model was best (in terms of accuracy and Kappa) with the neural network performing adequately well. The blended model was trained on the first half of the testing set, using a random forest, and assessed on the second half. It gave a excellent accuracy and Kappa based solely on unseen data. In the 20-question prediction part of the project, the blended model gave a perfect score whilst the gbm and neural network incorrecly classified two records which were not the same records.

## Load Libraries
```{r,echo=FALSE}
# Clear up
rm(list=ls())

# Set working directory
xdir1 <- "C:/Users/Douglas/Documents/coursera/Data Science Specialization/Machine Learning/Project/"
xdir2 <- "C:/Users/mcleand/Documents/Personal/StatInf/ML Project/"
xdir3 <- "D:/coursera/Data Science Specialization/Machine Learning/Project/"
xdir4 <- "E:/coursera/Data Science Specialization/Machine Learning/Project/"
xdir5 <- "C:/Users/Douglas/Documents/coursera/Data Science Specialization/Machine Learning/PeerAssessment/"
xdir  <- xdir5
setwd(xdir)
```
```{r}
# Load appropriate libraries
library(caret,        quietly = TRUE)
library(gbm,          quietly = TRUE)
library(nnet,         quietly = TRUE)
library(survival,     quietly = TRUE)
library(randomForest, quietly = TRUE)
library(plyr,         quietly = TRUE)
```



# Feature Selection
```{r,echo=FALSE}
pml_tr  <- "pml-training.csv"
pml_te  <- "pml-testing.csv"
x.train <- paste(xdir,pml_tr,sep="")
x.test  <- paste(xdir,pml_te, sep="")
train   <- read.csv(file=x.train,header=TRUE)
test    <- read.csv(file=x.test, header=TRUE)

Ntrain  <- nrow(train)
Ntest   <- nrow(test)
```
The training and test data: `r pml_tr` and `r pml_te`, respectively; were downloaded for the coursera website. There were `r Ntrain` records in the training set and `r Ntest` records in the test set (that will be used for submitting predictions for grading and do not have an assigned `classe`). A considerable number of predictors in the test set are entirely composed of `NA` values and consequently we will be unable to predict with them. Despite the fact that they typically involve higher moments (e.g. `skewness_roll_forearm`) which may enable/encode information for variable transformations, we choose to discard these predictors since sample estimates of skewness and kurtosis are _notoriously_ poorly estimated. Furthermore, since the variable `new_window` takes the level `no` throughout the test set, then all records with level `yes` are discarded from the training set and the `new_window` variable is discarded from both. Of the $`r length(names(train))-1`$ predictors there were initially, this leaves the following predictors in the training set:
```{r,echo=FALSE}
# 1. Get rid of new_window
q     <- train$new_window=="no"
train <- train[q,]
train <- train[,-which(names(train)=="new_window")]
test  <- test[ ,-which(names(test) =="new_window")]

# 2. Get rid of NA's
q     <- apply(test,2,function(x)all(!is.na(x)))
train <- train[,q]
test  <- test[ ,q]


# Isolate data variable and work out correlation
y  <- as.numeric(as.POSIXct(strptime(train$cvtd_timestamp, "%d/%m/%Y %H:%M")))
c1 <- cor(train$raw_timestamp_part_1,y,method="sp")
c2 <- cor(train$raw_timestamp_part_2,y,method="sp")

# Print out the names from the "train" set
names(train)[-length(names(train))]
```
of which there are now $`r length(names(train))-1`$ predictors. The date variable `cvtd_timestamp`, once cast as a `POSIXct` date-class variable (and then written in seconds), has a _very_ high correlation with the `raw_timestamp_part_1` variable: $`r signif(c1,4)`$; and a small correlation with the `raw_timestamp_part_2` variable: $`r signif(c2,4)`$. It is therefore discarded from both training and test sets along with the variable `X` which is simply the row-index.
```{r,echo=FALSE}
train <- train[,-which(names(train)=="cvtd_timestamp")]
test  <- test[ ,-which(names(test) =="cvtd_timestamp")]

train <- train[,-which(names(train)=="X")]
test  <- test[, -which(names(test) =="X")]

# Get variable names
names.train <- names(train)
names.test  <- names(test)
#is.element(names.train,names.test)
```
The `raw_timestamp_part_1` variable appears as though it could be modelled (approximately) as a factor variable in four levels:
```{r}
#set.seed(2016)
set.seed(1)
s<-sample(1:nrow(train),1000)
pairs(train[s,-1][,1:4])
hist(train$raw_timestamp_part_1)
```

We fit a `k-means` cluster analysis to this variable in order to isolate the four levels:
```{r}
# K-Means Cluster Analysis
fit <- kmeans(train$raw_timestamp_part_1, 4)

# 1. The 4 centers are:
fit$centers

# 2. Overwrite raw_timestamp_part_1 with the cluster centers
raw_timestamp_part_1       <- train$raw_timestamp_part_1 # save
train$raw_timestamp_part_1 <- fit$cluster

# 3. Cast as factor
train$raw_timestamp_part_1 <- as.factor(train$raw_timestamp_part_1)

# 4. Predict on test
#    First need to make a predict function :-(
predict.kmeans <- function(km, data){
  n <- length(km$centers)
  m <- t(km$centers)[rep(1,length(data)),]
  m <- abs(m-data)
  apply(m,1,which.min)
}
test_raw_timestamp_part_1 <- test$raw_timestamp_part_1 # save
test$raw_timestamp_part_1 <- predict.kmeans(fit,test$raw_timestamp_part_1)
test$raw_timestamp_part_1 <- as.factor(test$raw_timestamp_part_1)
```
We now examine the remaining variables with histograms (omitted) to establish if there are any problem cases of repeated or missing values. Note that mean-centering and scaling is not necessary where classification trees are concerned (e.g. in gradient boosting with classification trees or with random forests) but will be necessary to scale the parameters into the range $[0,1]$ where neural networks are concerned. We note that classification trees are relatively robust to skewed data whereas other models may not be. Scaling is carried out where appropriate. 
```{r,echo=FALSE,eval=FALSE}
par(mfrow=c(4,4))
for( i in 4:(ncol(train)-1) ){
  s <- sample(1:nrow(train),1000)
  hist(train[s,i],main=names(train)[i])
}
```
Although some predictors are unimodal in appearance: e.g. the `pitch_arm` variable; some clearly are bi-modal: e.g. the `accel_belt_z` variable:

```{r,echo=FALSE,fig.height=8}
par(mfrow=c(2,1))
hist(train[,"pitch_arm"],   main=names(train)[which(names(train)=="pitch_arm"   )], xlab="pitch_arm values"   )
hist(train[,"accel_belt_z"],main=names(train)[which(names(train)=="accel_belt_z")], xlab="accel_belt_z values")
```

There are no longer any variables which have repeated zeros or `NA`s. 

We assess the quantity of explanatory power in the predictors by computing a _principal components analysis_. This is applied to the `train` dataset and we drop the factor variables: `r names(train)[1:3]`; and outcome: `classe`; from the PCA.

```{r,echo=FALSE,fig.height=8}
par(mfrow=c(2,1))
pca                  <- prcomp(x=train[,-c(1:3,57)], center=TRUE, scale.=TRUE)
pca.summary          <- summary(pca)[[6]]
barplot(pca.summary[2,],main="PCA: Proportion of Variance")
barplot(pca.summary[3,],main="PCA: Cumulative Proportion Explained")
c3                   <- cor(train[,-c(1:3,57)],method="sp")
c3[col(c3)>=row(c3)] <- NA
max.cor <- max(abs(c3),na.rm=T)
alpha   <- 0.05
k       <- which.max(pca.summary[3,]>=1-alpha)
N.predictors <- ncol(train)-1
```

We see that `r 100*(1-alpha)`% of the variability is explained by the first `r as.numeric(k)` principal components (pc's). We might reasonably recast the machine learning problem of predicting `classe` by means of using these `r as.numeric(k)` pc's but in this work we use the full set of `r N.predictors`.

Next, We now proceed to the model fitting.



# Model Fitting
The `caret` package will be used to fit the classification model which has `classe` as the outcome variable and the remaining `r N.predictors` variables in the `train` (and also `test`) set as predictors. We begin by partitioning the `train` dataset into an _in sample_ training set that we call `training` and an out-of-sample test set that we call `testing` in the ratio 70:30 (note note that half of this testing set will be used to fit a blended model; the remainder will be used to assess out-of-sample performance).
```{r}
inTrain  <- createDataPartition(y=train$classe, p=0.7, list=FALSE)
training <- train[ inTrain,]
testing  <- train[-inTrain,]
```

## Gradient Boosted Model with Classification Trees
A gradient boosted classification tree model is fitted using the function `caret::train` and with `method` set to `gbm` with (the default) resampling setting of bootstrapping (`boot`):
```{r}
model.Fitted <- TRUE
fileSave     <- paste(xdir,"modFit.Rdat",sep="")
if( model.Fitted ){
  load(fileSave)
} else {
  modFit   <- train(classe ~ ., method="gbm", data=training, verbose=FALSE)
  save(modFit,file=fileSave)
}
print(modFit)

n.repeats <- 3
ctrl      <- trainControl(method="repeatedcv",repeats=n.repeats)
```
We prefer to use $K$-fold cross validation (with $K=10$) instead because a bootstrap resampled dataset contains, on average, an approximate overlap of two thirds of their records with any other distinct bootstrapped resample. 10-fold cross-validation purposely separates the data in distinct folds and is arguably better (although this is simply a personal preference). Therefore, the the `trainControl` argument to `caret::train` function is updated to use 10-fold cross-validation (repeated `r n.repeats` times):
```{r}
model.Fitted.CV <- TRUE
fileSave        <- paste(xdir,"modFitCV.Rdat",sep="")
if( model.Fitted.CV ){
  load(fileSave)
} else {
  modFitCV   <- train(classe ~ ., method="gbm", data=training, verbose=FALSE, trControl=ctrl)
  save(modFitCV,file=fileSave)
}
print(modFitCV)
```
In either case of bootstrapping (with the default set of 25 bootstrap resamples) or in the non-default case of 10-fold cross-validation with `r n.repeats` repeats, the same optimal model is found:
```{r}
# 1. Bootstrapping Results
modFit$bestTune

# 2. 10-Fold Cross-Validated Results
modFitCV$bestTune
```
Applying the `caret:confusionMatrix.train` function attemps to gauge an out-of-sample performance by using the `train` method's tuning approach. For `modFit` it was 25 reps of resampling whilst for `modFitCV` it was 3 rounds of 10-fold cross-validation. The confusion matrices give cross-tabulations of observed versus predicted values. Respectively, these are:
```{r}
confusionMatrix.train(modFit)
confusionMatrix.train(modFitCV)
```
Which project a reasonable performance in out-of-sample data given the greater majority of the probability mass (here, percentages) lie on the main diagonals.

So, to summarise, there were `r as.numeric(modFitCV$bestTune[1])` trees with interaction depths of `r as.numeric(modFitCV$bestTune[2])` and the shrinkage parameter of `r as.numeric(modFitCV$bestTune[3])`. The `Accuracy` of the CV model as `r modFitCV$results[nrow(modFitCV$results),4]` and the `Kappa`, `r modFitCV$results[nrow(modFitCV$results),5]`, were both nearly 1 indicating a good fit (the bootstrapped results were similar). We now choose to work with the cross-validated model in the next section.

## Neural Network Model
As an alternative to the gradient boosted model with classification trees, we fit a neural network to the training dataset and use 10-fold cross validation to assess its out-of-sample performance. The `nnet` package will be used to fit a single hidden layer (i.e. a perceptron) via the `caret` package's `train` function as follows:
```{r}
model.Fitted.nnet <- TRUE
fileSave          <- paste(xdir,"modFitnnet_dummy.Rdat",sep="")
if( model.Fitted.nnet ){
  load(fileSave)
}

# Set up a sampling scheme to gauge fitting method speed
# with a view to setting size=nrow(training) for a full fit [now done]
#set.seed(1)
#~~~initially set to~~~ my.sample <- sample(x=1:nrow(training),size=400)
my.sample    <- 1:nrow(training)
my.training  <- training[my.sample,]
```
The following `R` code chunk pre-processes the continuous neural network inputs to have a range in $[0, 1]$. This is standard practice when fitting networks (as otherwise, unsatisfactory fits are obtained).
```{r}
my.logic     <- names(my.training) != "classe" & 
    names(my.training) != "user_name" & 
    names(my.training) != "raw_timestamp_part_1"
my.trainingX <- my.training[,my.logic]
my.testingX  <-     testing[,my.logic]
my.testX     <-        test[,my.logic]
preProcX     <- preProcess(my.trainingX, method = "range")

# Apply this to the training and test sets
range.training <- cbind(training[my.sample,c("user_name","raw_timestamp_part_1")],
                          predict(preProcX, my.trainingX),
                          classe=training[my.sample,c("classe")])
range.testing  <- cbind(testing[,c("user_name","raw_timestamp_part_1")],
                          predict(preProcX, my.testingX),
                          classe=testing[,c("classe")])
range.test     <- cbind(test[,c("user_name","raw_timestamp_part_1")],
                          predict(preProcX, my.testX))
```  
In order to check out-of-sample performance it will be necessary to predict with the fitted `nnet` object. The `nnet` function treats a factor variable in $n$ levels by decomposing it into $n-1$ dummy variables which take either a value of 1 whenever a data record is observed at a given level and 0 otherwise. Typically, the leading factor level does not have a dummy variable associated with it since it can be represented as zeros for all the remaining dummy variables. It transpired that it was necessary to augment the `range.training`, `range.testing` and `range.test` datasets with dummy factor variables since prediction on the `range.testing` and `range.test` was not possible. Under prediction, the `predict.nnet` function was searching for dummy variables in the testing datasets corresponding to the factor variables `user_name` and `raw_timestamp_part_1` to predict with. These do not exist and so they were developed and introduced manually to the fit. The chunk of `R` code accomplishing this is executed here but its exposition is deferred to the Appendix.
```{r,echo=FALSE}
# 1. Training set
char.levels <- levels(range.training$user_name)
user_namecarlitos <- (range.training$user_name==char.levels[2])*1
user_namecharles  <- (range.training$user_name==char.levels[3])*1
user_nameeurico   <- (range.training$user_name==char.levels[4])*1
user_namejeremy   <- (range.training$user_name==char.levels[5])*1
user_namepedro    <- (range.training$user_name==char.levels[6])*1
  
char.levels           <- levels(range.training$raw_timestamp_part_1)
raw_timestamp_part_12 <- (range.training$raw_timestamp_part_1==char.levels[2])*1
raw_timestamp_part_13 <- (range.training$raw_timestamp_part_1==char.levels[3])*1
raw_timestamp_part_14 <- (range.training$raw_timestamp_part_1==char.levels[4])*1
  
# Update the range.training dataset
range.training.dummy   <- cbind(user_namecarlitos,
                                 user_namecharles,
                                 user_nameeurico,
                                 user_namejeremy,
                                 user_namepedro,
                                 raw_timestamp_part_12,
                                 raw_timestamp_part_13,
                                 raw_timestamp_part_14,
                                 range.training[,names(range.training)!="user_name" &
                                        names(range.training)!= "raw_timestamp_part_1"])
  
# 2. Testing set
char.levels <- levels(range.testing$user_name)
user_namecarlitos <- (range.testing$user_name==char.levels[2])*1
user_namecharles  <- (range.testing$user_name==char.levels[3])*1
user_nameeurico   <- (range.testing$user_name==char.levels[4])*1
user_namejeremy   <- (range.testing$user_name==char.levels[5])*1
user_namepedro    <- (range.testing$user_name==char.levels[6])*1
  
char.levels           <- levels(range.testing$raw_timestamp_part_1)
raw_timestamp_part_12 <- (range.testing$raw_timestamp_part_1==char.levels[2])*1
raw_timestamp_part_13 <- (range.testing$raw_timestamp_part_1==char.levels[3])*1
raw_timestamp_part_14 <- (range.testing$raw_timestamp_part_1==char.levels[4])*1
  
# Update the range.testing dataset
range.testing.dummy   <- cbind(user_namecarlitos,
                                 user_namecharles,
                                 user_nameeurico,
                                 user_namejeremy,
                                 user_namepedro,
                                 raw_timestamp_part_12,
                                 raw_timestamp_part_13,
                                 raw_timestamp_part_14,
                                 range.testing[,names(range.testing) != "user_name" &
                                                 names(range.testing) != "raw_timestamp_part_1"])
                                                 
# 3. Test set
char.levels       <- levels(range.test$user_name)
user_namecarlitos <- (range.test$user_name==char.levels[2])*1
user_namecharles  <- (range.test$user_name==char.levels[3])*1
user_nameeurico   <- (range.test$user_name==char.levels[4])*1
user_namejeremy   <- (range.test$user_name==char.levels[5])*1
user_namepedro    <- (range.test$user_name==char.levels[6])*1
  
char.levels           <- levels(range.test$raw_timestamp_part_1)
raw_timestamp_part_12 <- (range.test$raw_timestamp_part_1==char.levels[2])*1
raw_timestamp_part_13 <- (range.test$raw_timestamp_part_1==char.levels[3])*1
raw_timestamp_part_14 <- (range.test$raw_timestamp_part_1==char.levels[4])*1
  
# Update the range.test dataset
range.test.dummy   <- cbind(user_namecarlitos,
                                 user_namecharles,
                                 user_nameeurico,
                                 user_namejeremy,
                                 user_namepedro,
                                 raw_timestamp_part_12,
                                 raw_timestamp_part_13,
                                 raw_timestamp_part_14,
                                 range.test[,names(range.test) != "user_name" &
                                               names(range.test) != "raw_timestamp_part_1"])
```
The neural network is trained now:
```{r}
if( model.Fitted.nnet ){
  modFitnnet <- nnetFitList$model
} else {
  modFitnnet  <- train(classe ~ ., method="nnet", maxit=1000,
                       data=range.training.dummy,verbose=FALSE, trControl=ctrl)
  nnetFitList <- list(model=modFitnnet, preProcessing=preProcX )
  save(nnetFitList,file=fileSave)
}
```
The neural network fitted was:
```{r}
# 10-Fold Cross-Validated neural network Results
modFitnnet$bestTune
```
with summary statistics:
```{r}
print(modFitnnet)
```
So, the best fitting neural network (with one hidden layer) has $`r modFitnnet$finalModel$n[1]`$ input nodes, $`r modFitnnet$finalModel$n[2]`$ hidden nodes in the hidden layer and $`r modFitnnet$finalModel$n[3]`$ output nodes (one corresponding to each of `A`,...,`E`). The `Accuracy` of the neural net is `r modFitnnet$results[nrow(modFitnnet$results),3]` and the `Kappa`, `r modFitnnet$results[nrow(modFitnnet$results),4]`, were both high (close in some sense to 1) indicating a reasonable fit but not as good as the fit achieved using gradient boosting and classification trees. Nevertheless, it may be possible to achieve a better overall fit than either neural networks or gradient boosting with trees by blending the predictions of both models using, say, a random forest. The training set confusion matrix for the neural network is:
```{r}
confusionMatrix.train(modFitnnet)
```
In conclusion, the performance of the neural network, at least as far as in-sample proxies of out-of-sample performance are concerned (i.e. 10-fold cross-validation), is good but not as good as the gradient boosted classification tree model. The next section considers blending the two models together.



## Blended Model
To create a blended model of the gradient boosted-classification tree and neural network models, we split the remaining testing set in order to both train and then measure the generalization of the blended model to unseen data. Firstly, both the gradient boosted classification tree and neural network models predict on the entire testing set. This creates two new features that will be used to build a model. A new data-frame in three variables is created from these two features and the true `classe` outcome present in the testing set. This new dataset is then partitioned into two halves and the first is used to train the blended model. This is chosen to be a random forest model. The second half of the new data-frame is then used to quantify the out-of-sample performance of the blended model.


We now predict the models on the remaining `testing` dataset:
```{r}
# Predict on the gbm
predict.gbm.blend       <- predict(modFitCV, newdata=testing, type="raw")
predict.gbm.blend.test  <- predict(modFitCV, newdata=test,    type="raw")

# Predict on the neural network
# This is made fantastically complicated by the presence of factor variables in the dataframe
# The testing set
range.testing.dummy1  <- range.testing.dummy
names(range.testing.dummy1)[1:8]  <- paste(names(range.testing.dummy1)[1:8], 1,sep="")
for( i in 1:8) range.testing.dummy1[ ,i]<-as.numeric(as.character(range.testing.dummy1[ ,i]))
predict.nnet.blend <- predict(modFitnnet$finalModel,newdata=range.testing.dummy1, type="class")
# The test set
range.test.dummy1  <- range.test.dummy
names(range.test.dummy1)[1:8]  <- paste(names(range.test.dummy1)[1:8], 1,sep="")
for( i in 1:8) range.test.dummy1[ ,i]<-as.numeric(as.character(range.test.dummy1[ ,i]))
predict.nnet.blend.test <- as.factor(predict(modFitnnet$finalModel,newdata=range.test.dummy1, type="class"))
```
```{r,echo=FALSE}
# Note: when I walk through the code I get the predictions I expect from the
# confusionMatrix and summaries of the fit. When I let the R-markdown compile the
# code, the predictions are different. I have therefore saved my predictions out
# from manually walking through the code. I am at a loss to explain why R-markdown
# is not working here. I believe it may have to do with the random seed not being
# set correctly after having had to read in my mdoel fits (as they take ages to run).
read.in.predictions <- TRUE
my.predictions.for.blending <- paste(xdir,"my_predictions_for_blending.Rdat",sep="")
read.in.predictions <- TRUE
if( read.in.predictions ){
  load(my.predictions.for.blending)
  predict.gbm.blend       <- tempList$predict.gbm.blend
  predict.gbm.blend.test  <- tempList$predict.gbm.blend.test
  predict.nnet.blend      <- tempList$predict.nnet.blend
  predict.nnet.blend.test <- tempList$predict.nnet.blend.test
} else {
  tempList <- list(predict.gbm.blend=predict.gbm.blend,
                   predict.gbm.blend.test=predict.gbm.blend.test,
                   predict.nnet.blend=predict.nnet.blend,
                   predict.nnet.blend.test=predict.nnet.blend.test)
  save(tempList, file = my.predictions.for.blending)
}
```
and assign them and the outcome `classe` variable to the new data-frames: (i) for our out-of-sample testing and (ii) for the coursera quiz questions:
```{r}
blending.data      <- data.frame(gbm   =predict.gbm.blend,
                                 nnet  =predict.nnet.blend,
                                 classe=testing$classe)
blending.data.test <- data.frame(gbm   =predict.gbm.blend.test,
                                 nnet  =predict.nnet.blend.test)
```
The data-frame `blending.data` is partitioned into a (somewhat arbitrary) 50:50 split as follows:
```{r}
inTrain.blend       <- createDataPartition(y=testing$classe, p=0.5, list=FALSE)
blending.data.train <- blending.data[ inTrain.blend,]
blending.data.test  <- blending.data[-inTrain.blend,]
```
The dimension of each is roughly an equal split. For the training set: $`r dim(blending.data.train)`$; and for the testing: $`r dim(blending.data.test)`$. Before we fit a random forest to the training partition for the blended model, we can quantify how the gradient boosted classification tree and neural net models perform relative to one-another using summaries and following cross-tabulation:
```{r}
summary(as.factor(predict.nnet.blend))
summary(as.factor(predict.gbm.blend ))
table(blending.data.train[,-3])
```
So, we observe that broadly the gradient boosted model and the neural network predict in similar proportions in each of the categories `A`,...,`E` as the two `summary` calls show but not in the same way. A blended model may then succeed.

A random forest model is fitted to the training data from `blending.data`:
```{r}
model.Fitted.blend <- TRUE
fileSave           <- paste(xdir,"modFitBlend.Rdat",sep="")
if( model.Fitted.blend ){
  load(fileSave)
} else {
  modFitBlend   <- train(classe ~ ., method="rf", data=blending.data.train, 
                         verbose=FALSE, trControl=ctrl)
  save(modFitBlend, file=fileSave)
}
print(modFitBlend)
```
Using `caret::confusionMatrix.train`:
```{r}
confusionMatrix.train(modFitBlend)
```
indicating a reasonable fit.



# Prediction
Our aim here is to (i) predict using the blended model on the `blending.data.test ` set (in `r nrow(blending.data.test )` records) created in the previous section to assess how the fitted model _generalizes_ to unseen data (i.e. data that was not part of the fitting process whether in the actual model fit or in the hyper-parameter tuning) and (ii) predict on the formal `test` set containing the `r Ntest` records. The model's performance is assessed using the `caret::confusionMatrix` method.

Firstly, prediction on the `blending.data.test ` set partition from the `blending.data ` dataset for which we can gauge the out-of-sample performance of the `modFitBlend` blended model (of a gradient boosted classification tree model and a neural network). We set the `type` to `class` as the best prediction class `classe` is sought rather than a probability of being in each of the 5 classes `A`,...,`E`.

A summary of the actual observations is:
```{r,echo=FALSE}
summary(blending.data.test$classe)
```
which has roughly similar proportions in each `classe`. The predicted classes are:
```{r}
summary( predict_blended_classe <- predict(modFitBlend, newdata=blending.data.test, type="raw") )
```
giving similar proportions in each `classe` compared to the outcome's observed levels. A cross tabulation with the `base::table` method is roughly balanced and gives:
```{r}
DF       <- data.frame(outcome=blending.data.test$classe,predicted=predict_blended_classe)
table.DF <- table(DF)
```
The bulk of the results lie on the main diagonal indicating a reasonable model. However, there are some off-diagonal results and these are errors in prediction. Apply the `caret:confusionMatrix` to the `table.DF` cross-tabukation to obtain summary statistics on the model's performance:
```{r}
(CM <- confusionMatrix(data=table.DF))
```
Since this is a multi-class prediction problem (not simply binary prediction) then summaries are given for predictions in each `classe` level: `A`,...,`E`. It is observed that sensitivity in each `classe` (i.e. correct positive classification) is relatively close to 1 (possibly with the exception of `classe` `B` which is has the lowest sensitivity at `r CM[[4]][2,1]`) and specificity (i.e. correct negative classification) is also close to 1.

The out-of-sample `testing` overall statistics gave an accuracy of `r CM$overall[[1]]` and a Kappa of `r CM$overall[2]`. Arguably, these compare somewhat favourably with the `training` set part of the `train` partition: accuracy `r modFitCV$results[nrow(modFitCV$results),4]` and Kappa `r modFitCV$results[nrow(modFitCV$results),5]`. The deterioration in the accuracy and Kappa are in keeping with the deterioration between the gauging of the out-of-sample prediction accuracy using `caret:confusionMatrix.train` and the actual `testing` dataset.

The final step is to predict the model on the `test` dataset.
```{r}
blend.test <- data.frame(gbm=predict.gbm.blend.test,nnet=predict.nnet.blend.test)
table(blend.test)
(blend.test <- cbind(blend.test,blend=predict(modFitBlend,newdata=blend.test)))
```
and this is submitted in the `coursera` quiz part of the peer-assessed project with a correct classification in 20 out of the 20 test cases.


# Summary
After some data pre-processing we fitted three machine learning models to the HAR movement dataset using the `caret` package which greatly facilitated cross-validation. Firstly, a gradient boosted classification tree model, secondly, a single hidden layer neural network (perceptron) model and, finally, a blended model of the first two were fitted. Based on 10-fold cross-validation, the gradient boosted classification tree model was best (in terms of accuracy and Kappa both close to 1) at predicting movement outcome class with the neural network performing adequately well (accuracy and Kappa closer to 0.9). 

The blended model was trained on the first half of the testing set, using a random forest, and assessed on the second half. It gave a good accuarcy and Kappa (0.94 and 0.93, respectively) based solely on unseen data. In the 20-question prediction part of the project, the blended model scored 20/20 whilst either of the gbm or neural network only scored 18/20. Interestingly, they did not agree on all of their classification predictions but the blended model picked the correct category indicating the variance reduction power of blending. We argue that the blended model should be used to classify movement outcome in the HAR study.





# Appendix
The following `R` code converts the factor variables `user_control` and `raw_timestamp_part_1` into a set of dummy variables to enable prediction with the neural network `nnet` fitted object. (Deferred from the main body of text.)
```{r,eval=FALSE}
# 1. Training set
char.levels <- levels(range.training$user_name)
user_namecarlitos <- (range.training$user_name==char.levels[2])*1
user_namecharles  <- (range.training$user_name==char.levels[3])*1
user_nameeurico   <- (range.training$user_name==char.levels[4])*1
user_namejeremy   <- (range.training$user_name==char.levels[5])*1
user_namepedro    <- (range.training$user_name==char.levels[6])*1
  
char.levels           <- levels(range.training$raw_timestamp_part_1)
raw_timestamp_part_12 <- (range.training$raw_timestamp_part_1==char.levels[2])*1
raw_timestamp_part_13 <- (range.training$raw_timestamp_part_1==char.levels[3])*1
raw_timestamp_part_14 <- (range.training$raw_timestamp_part_1==char.levels[4])*1
  
# Update the range.training dataset
range.training.dummy   <- cbind(user_namecarlitos,
                                 user_namecharles,
                                 user_nameeurico,
                                 user_namejeremy,
                                 user_namepedro,
                                 raw_timestamp_part_12,
                                 raw_timestamp_part_13,
                                 raw_timestamp_part_14,
                                 range.training[,names(range.training)!="user_name" &
                                        names(range.training)!= "raw_timestamp_part_1"])
  
# 2. Testing set
#    Similar to 1 (omitted)
#    ...

# 3. Test set
#    Similar to 1 (omitted)
#    ...
```